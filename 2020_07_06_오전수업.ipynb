{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020_07_06_오전수업.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4z1mr1OIjRROrpKIc1p2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturu1/github-first/blob/master/2020_07_06_%EC%98%A4%EC%A0%84%EC%88%98%EC%97%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBW4fVaq1x6w",
        "colab_type": "text"
      },
      "source": [
        "# SimpleRNN 레이어     176페이지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xCk58Bbgcsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b9b0f059-9574-4cef-c254-d0124f0e28cc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(6):\n",
        "  lst = list(range(i, i+4))\n",
        "  X.append(list(map(lambda c: [c/10], lst)))\n",
        "  Y.append((i+4)/10)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "for i in range(len(X)):\n",
        "  print(X[i], Y[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. ]\n",
            " [0.1]\n",
            " [0.2]\n",
            " [0.3]] 0.4\n",
            "[[0.1]\n",
            " [0.2]\n",
            " [0.3]\n",
            " [0.4]] 0.5\n",
            "[[0.2]\n",
            " [0.3]\n",
            " [0.4]\n",
            " [0.5]] 0.6\n",
            "[[0.3]\n",
            " [0.4]\n",
            " [0.5]\n",
            " [0.6]] 0.7\n",
            "[[0.4]\n",
            " [0.5]\n",
            " [0.6]\n",
            " [0.7]] 0.8\n",
            "[[0.5]\n",
            " [0.6]\n",
            " [0.7]\n",
            " [0.8]] 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZAFLisZuYdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "139af6ed-82fd-4454-acce-4b80d6b1e475"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]),\n",
        "                             tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 10)                120       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 131\n",
            "Trainable params: 131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsxEps5t2ofh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cd47c713-9d3f-4faa-d00f-94272f936a5d"
      },
      "source": [
        "model.fit(X, Y, epochs=100, verbose=0)\n",
        "print(model.predict(X))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.4031669 ]\n",
            " [0.5441784 ]\n",
            " [0.65340364]\n",
            " [0.7308126 ]\n",
            " [0.7816807 ]\n",
            " [0.81260735]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig0_Ql6Q24L6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4099ba28-e4ab-4c93-ed7d-6889fc3e19d9"
      },
      "source": [
        "print(model.predict(np.array([[[0.6], [0.7], [0.8], [0.9]]])))\n",
        "print(model.predict(np.array([[[-0.1], [0.0], [0.1], [0.2]]])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.829411]]\n",
            "[[0.23974644]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znQOA7Of4GAe",
        "colab_type": "text"
      },
      "source": [
        "# LSTM 레이어  181페이지\n",
        "\n",
        "1. cell gate 기억\n",
        "2. forget gate \n",
        "3. input gate 새롭게 추출한 특징\n",
        "4. output gate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkjG1K9w9BId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35fca891-9ca1-4b16-d229-9e671398a852"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(3000):\n",
        "  lst = np.random.rand(100)\n",
        "  idx = np.random.choice(100, 2, replace=False)\n",
        "  zeros = np.zeros(100)\n",
        "  zeros[idx] = 1\n",
        "  X.append(np.array(list(zip(zeros, lst))))\n",
        "  Y.append(np.prod(lst[idx]))\n",
        "\n",
        "print(X[0], Y[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.55814868]\n",
            " [1.         0.67532271]\n",
            " [0.         0.95121581]\n",
            " [0.         0.68468502]\n",
            " [0.         0.24193717]\n",
            " [0.         0.99926464]\n",
            " [0.         0.90130643]\n",
            " [0.         0.02088023]\n",
            " [0.         0.40424586]\n",
            " [0.         0.70539667]\n",
            " [0.         0.80241406]\n",
            " [0.         0.370403  ]\n",
            " [0.         0.17728187]\n",
            " [0.         0.7962993 ]\n",
            " [0.         0.79373351]\n",
            " [0.         0.20193479]\n",
            " [0.         0.98920967]\n",
            " [0.         0.24009197]\n",
            " [0.         0.58453064]\n",
            " [0.         0.13962583]\n",
            " [0.         0.8484645 ]\n",
            " [0.         0.36580341]\n",
            " [0.         0.84099191]\n",
            " [0.         0.66411087]\n",
            " [0.         0.85201823]\n",
            " [0.         0.82567313]\n",
            " [0.         0.99606265]\n",
            " [0.         0.12258626]\n",
            " [1.         0.26821638]\n",
            " [0.         0.57258986]\n",
            " [0.         0.12813634]\n",
            " [0.         0.04283146]\n",
            " [0.         0.91038451]\n",
            " [0.         0.43646716]\n",
            " [0.         0.46277671]\n",
            " [0.         0.89606956]\n",
            " [0.         0.22265081]\n",
            " [0.         0.28326651]\n",
            " [0.         0.37875164]\n",
            " [0.         0.9572551 ]\n",
            " [0.         0.4484715 ]\n",
            " [0.         0.10131248]\n",
            " [0.         0.42266128]\n",
            " [0.         0.66292312]\n",
            " [0.         0.01827281]\n",
            " [0.         0.62333785]\n",
            " [0.         0.29312768]\n",
            " [0.         0.6977162 ]\n",
            " [0.         0.99423269]\n",
            " [0.         0.53197383]\n",
            " [0.         0.57119045]\n",
            " [0.         0.37903309]\n",
            " [0.         0.3624022 ]\n",
            " [0.         0.15985373]\n",
            " [0.         0.05781223]\n",
            " [0.         0.27003961]\n",
            " [0.         0.89267649]\n",
            " [0.         0.43346762]\n",
            " [0.         0.19032386]\n",
            " [0.         0.5865373 ]\n",
            " [0.         0.25423121]\n",
            " [0.         0.19767049]\n",
            " [0.         0.88371805]\n",
            " [0.         0.49500188]\n",
            " [0.         0.21301297]\n",
            " [0.         0.70021516]\n",
            " [0.         0.54440564]\n",
            " [0.         0.11397297]\n",
            " [0.         0.26635076]\n",
            " [0.         0.32150111]\n",
            " [0.         0.98878672]\n",
            " [0.         0.27645639]\n",
            " [0.         0.13720725]\n",
            " [0.         0.75604082]\n",
            " [0.         0.61608758]\n",
            " [0.         0.24090676]\n",
            " [0.         0.69974173]\n",
            " [0.         0.70877155]\n",
            " [0.         0.92530142]\n",
            " [0.         0.74027088]\n",
            " [0.         0.46390203]\n",
            " [0.         0.65569629]\n",
            " [0.         0.94412078]\n",
            " [0.         0.75198702]\n",
            " [0.         0.72693706]\n",
            " [0.         0.77750681]\n",
            " [0.         0.39032342]\n",
            " [0.         0.37343854]\n",
            " [0.         0.47200267]\n",
            " [0.         0.0824572 ]\n",
            " [0.         0.41341317]\n",
            " [0.         0.10985475]\n",
            " [0.         0.17673248]\n",
            " [0.         0.0101375 ]\n",
            " [0.         0.76207144]\n",
            " [0.         0.99179282]\n",
            " [0.         0.63152061]\n",
            " [0.         0.1491169 ]\n",
            " [0.         0.55055765]\n",
            " [0.         0.89001896]] 0.18113261500100014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x8GvBg-9BF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f49c865-06db-4caa-db82-451ddb3c3569"
      },
      "source": [
        "print(len(X), len(Y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuITbP_S3htv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "12c88428-f890-4ce2-d1de-cf7eaffa6543"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "                             tf.keras.layers.SimpleRNN(units=30),\n",
        "                             tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_5 (SimpleRNN)     (None, 100, 30)           990       \n",
            "_________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)     (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 2,851\n",
            "Trainable params: 2,851\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_exx5DAC08i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86f4080b-8eec-42ca-dfde-478936b5ff12"
      },
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0270 - val_loss: 0.0725\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0268 - val_loss: 0.0720\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0254 - val_loss: 0.0719\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0255 - val_loss: 0.0732\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0249 - val_loss: 0.0720\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 2s 35ms/step - loss: 0.0251 - val_loss: 0.0704\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0243 - val_loss: 0.0713\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0244 - val_loss: 0.0725\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0236 - val_loss: 0.0715\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0230 - val_loss: 0.0695\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0228 - val_loss: 0.0740\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0226 - val_loss: 0.0736\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0240 - val_loss: 0.0758\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0232 - val_loss: 0.0766\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 2s 34ms/step - loss: 0.0224 - val_loss: 0.0721\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0215 - val_loss: 0.0744\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0221 - val_loss: 0.0734\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0231 - val_loss: 0.0723\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0210 - val_loss: 0.0716\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0212 - val_loss: 0.0757\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0214 - val_loss: 0.0755\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0204 - val_loss: 0.0761\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0197 - val_loss: 0.0756\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0213 - val_loss: 0.0773\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0195 - val_loss: 0.0788\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0203 - val_loss: 0.0767\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0207 - val_loss: 0.0791\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0192 - val_loss: 0.0727\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0192 - val_loss: 0.0720\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0188 - val_loss: 0.0804\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0190 - val_loss: 0.0743\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0184 - val_loss: 0.0777\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0183 - val_loss: 0.0770\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0189 - val_loss: 0.0831\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0181 - val_loss: 0.0796\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0175 - val_loss: 0.0766\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 2s 34ms/step - loss: 0.0171 - val_loss: 0.0782\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0164 - val_loss: 0.0775\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0168 - val_loss: 0.0787\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0167 - val_loss: 0.0766\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0165 - val_loss: 0.0780\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0157 - val_loss: 0.0761\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0166 - val_loss: 0.0812\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0170 - val_loss: 0.0839\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0168 - val_loss: 0.0791\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0162 - val_loss: 0.0801\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0155 - val_loss: 0.0803\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0155 - val_loss: 0.0794\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0141 - val_loss: 0.0834\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0148 - val_loss: 0.0825\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0160 - val_loss: 0.0789\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0155 - val_loss: 0.0821\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0146 - val_loss: 0.0826\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0162 - val_loss: 0.0783\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0156 - val_loss: 0.0808\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0141 - val_loss: 0.0824\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0155 - val_loss: 0.0806\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0151 - val_loss: 0.0817\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0130 - val_loss: 0.0822\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 2s 34ms/step - loss: 0.0145 - val_loss: 0.0784\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0141 - val_loss: 0.0862\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0138 - val_loss: 0.0801\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0139 - val_loss: 0.0814\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0125 - val_loss: 0.0788\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0136 - val_loss: 0.0861\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 2s 34ms/step - loss: 0.0131 - val_loss: 0.0849\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0131 - val_loss: 0.0861\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0125 - val_loss: 0.0785\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0125 - val_loss: 0.0853\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0128 - val_loss: 0.0798\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0125 - val_loss: 0.0827\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0149 - val_loss: 0.0859\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0139 - val_loss: 0.0809\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0125 - val_loss: 0.0856\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0133 - val_loss: 0.0844\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0128 - val_loss: 0.0826\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0123 - val_loss: 0.0890\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0149 - val_loss: 0.0854\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0128 - val_loss: 0.0844\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0117 - val_loss: 0.0851\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0113 - val_loss: 0.0837\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0111 - val_loss: 0.0881\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 2s 34ms/step - loss: 0.0117 - val_loss: 0.0857\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0170 - val_loss: 0.0846\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0130 - val_loss: 0.0802\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0123 - val_loss: 0.0801\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0107 - val_loss: 0.0808\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0114 - val_loss: 0.0847\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0109 - val_loss: 0.0865\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0112 - val_loss: 0.0866\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0106 - val_loss: 0.0856\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0136 - val_loss: 0.0876\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 0.0111 - val_loss: 0.0858\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 2s 33ms/step - loss: 0.0098 - val_loss: 0.0845\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0108 - val_loss: 0.0854\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0119 - val_loss: 0.0864\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0103 - val_loss: 0.0871\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0102 - val_loss: 0.0837\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.0107 - val_loss: 0.0823\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 0.0117 - val_loss: 0.0849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p634Phr6DjEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "099168ed-b896-4a6d-a6f9-ed2dde164213"
      },
      "source": [
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction = model.predict(X[2560:2560+5])\n",
        "\n",
        "for i in range(5):\n",
        "  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "\n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "    fail += 1\n",
        "\n",
        "print('correctness:', (440 - fail) / 440 * 100, '%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0831\n",
            "0.037643116130860715 \t 0.1024338 \tdiff: 0.06479068456646594\n",
            "0.7566858754751916 \t 0.5502948 \tdiff: 0.20639105898120358\n",
            "0.7462236268725224 \t 0.67629015 \tdiff: 0.06993347241543013\n",
            "0.31013333792165165 \t 0.31372964 \tdiff: 0.0035963059000646558\n",
            "0.5885166101087551 \t 0.17775899 \tdiff: 0.4107576183904629\n",
            "correctness: 10.909090909090908 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtXPQ5U9FW9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10f67a9e-d226-49b3-b3a7-23e5adfda13f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(3000):\n",
        "  lst = np.random.rand(100)\n",
        "  idx = np.random.choice(100, 2, replace=False)\n",
        "  zeros = np.zeros(100)\n",
        "  zeros[idx] = 1\n",
        "  X.append(np.array(list(zip(zeros, lst))))\n",
        "  Y.append(np.prod(lst[idx]))\n",
        "\n",
        "print(X[0], Y[0])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "                             tf.keras.layers.LSTM(units=30),\n",
        "                             tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)\n",
        "\n",
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction = model.predict(X[2560:2560+5])\n",
        "\n",
        "for i in range(5):\n",
        "  print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "\n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "  if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "    fail += 1\n",
        "\n",
        "print('correctness:', (440 - fail) / 440 * 100, '%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 2.09654015e-01]\n",
            " [0.00000000e+00 3.04666181e-01]\n",
            " [0.00000000e+00 2.20684024e-01]\n",
            " [1.00000000e+00 2.80213731e-01]\n",
            " [0.00000000e+00 8.46930924e-01]\n",
            " [0.00000000e+00 9.96994103e-01]\n",
            " [0.00000000e+00 8.90763520e-01]\n",
            " [0.00000000e+00 5.32093707e-01]\n",
            " [0.00000000e+00 1.47319459e-01]\n",
            " [0.00000000e+00 7.95796704e-02]\n",
            " [0.00000000e+00 4.23314091e-01]\n",
            " [0.00000000e+00 5.96554439e-01]\n",
            " [0.00000000e+00 5.61871783e-01]\n",
            " [0.00000000e+00 9.34513609e-01]\n",
            " [0.00000000e+00 5.05740982e-01]\n",
            " [0.00000000e+00 4.48465928e-01]\n",
            " [0.00000000e+00 1.17079305e-01]\n",
            " [0.00000000e+00 5.28178468e-01]\n",
            " [0.00000000e+00 1.10219750e-01]\n",
            " [1.00000000e+00 7.86551725e-01]\n",
            " [0.00000000e+00 2.57548360e-01]\n",
            " [0.00000000e+00 1.83292915e-01]\n",
            " [0.00000000e+00 8.96684163e-01]\n",
            " [0.00000000e+00 8.93632668e-01]\n",
            " [0.00000000e+00 2.49107059e-01]\n",
            " [0.00000000e+00 8.69799726e-01]\n",
            " [0.00000000e+00 2.89523897e-01]\n",
            " [0.00000000e+00 4.66459047e-02]\n",
            " [0.00000000e+00 3.04869661e-01]\n",
            " [0.00000000e+00 3.36735924e-01]\n",
            " [0.00000000e+00 6.57943946e-01]\n",
            " [0.00000000e+00 5.40198572e-01]\n",
            " [0.00000000e+00 8.29687939e-01]\n",
            " [0.00000000e+00 1.76607272e-01]\n",
            " [0.00000000e+00 2.75668954e-02]\n",
            " [0.00000000e+00 4.44957346e-02]\n",
            " [0.00000000e+00 3.40233049e-01]\n",
            " [0.00000000e+00 7.00801299e-01]\n",
            " [0.00000000e+00 5.83120471e-01]\n",
            " [0.00000000e+00 3.67010300e-01]\n",
            " [0.00000000e+00 2.84929894e-01]\n",
            " [0.00000000e+00 5.55266024e-01]\n",
            " [0.00000000e+00 2.35064018e-01]\n",
            " [0.00000000e+00 4.81540071e-01]\n",
            " [0.00000000e+00 2.50854232e-01]\n",
            " [0.00000000e+00 6.87751783e-01]\n",
            " [0.00000000e+00 7.25347684e-01]\n",
            " [0.00000000e+00 9.02679343e-02]\n",
            " [0.00000000e+00 9.27765797e-01]\n",
            " [0.00000000e+00 6.80830811e-02]\n",
            " [0.00000000e+00 2.94808842e-01]\n",
            " [0.00000000e+00 1.58448874e-01]\n",
            " [0.00000000e+00 6.60701464e-01]\n",
            " [0.00000000e+00 2.55378641e-01]\n",
            " [0.00000000e+00 9.80262700e-01]\n",
            " [0.00000000e+00 7.88335354e-01]\n",
            " [0.00000000e+00 2.93070685e-02]\n",
            " [0.00000000e+00 7.36379003e-01]\n",
            " [0.00000000e+00 8.97232724e-01]\n",
            " [0.00000000e+00 4.28751564e-02]\n",
            " [0.00000000e+00 5.68152921e-04]\n",
            " [0.00000000e+00 8.35409655e-01]\n",
            " [0.00000000e+00 2.31323360e-01]\n",
            " [0.00000000e+00 2.74171571e-01]\n",
            " [0.00000000e+00 2.55511243e-01]\n",
            " [0.00000000e+00 4.83806171e-01]\n",
            " [0.00000000e+00 2.57560532e-01]\n",
            " [0.00000000e+00 7.08280596e-02]\n",
            " [0.00000000e+00 1.41837598e-01]\n",
            " [0.00000000e+00 5.26989717e-01]\n",
            " [0.00000000e+00 6.05927637e-02]\n",
            " [0.00000000e+00 4.43274512e-01]\n",
            " [0.00000000e+00 1.20955106e-01]\n",
            " [0.00000000e+00 6.56151115e-01]\n",
            " [0.00000000e+00 6.84821862e-01]\n",
            " [0.00000000e+00 1.95587270e-01]\n",
            " [0.00000000e+00 3.38139798e-01]\n",
            " [0.00000000e+00 2.75100886e-01]\n",
            " [0.00000000e+00 1.73648886e-01]\n",
            " [0.00000000e+00 3.51627116e-01]\n",
            " [0.00000000e+00 1.03060732e-01]\n",
            " [0.00000000e+00 3.29253162e-01]\n",
            " [0.00000000e+00 8.38478880e-01]\n",
            " [0.00000000e+00 4.68573398e-01]\n",
            " [0.00000000e+00 1.54242316e-01]\n",
            " [0.00000000e+00 8.53431777e-01]\n",
            " [0.00000000e+00 5.80352155e-01]\n",
            " [0.00000000e+00 9.61934143e-01]\n",
            " [0.00000000e+00 3.82349726e-01]\n",
            " [0.00000000e+00 3.73066883e-01]\n",
            " [0.00000000e+00 1.97320230e-01]\n",
            " [0.00000000e+00 5.28975626e-01]\n",
            " [0.00000000e+00 4.86114315e-01]\n",
            " [0.00000000e+00 3.57020554e-01]\n",
            " [0.00000000e+00 3.35464803e-01]\n",
            " [0.00000000e+00 1.42365657e-01]\n",
            " [0.00000000e+00 1.23661369e-01]\n",
            " [0.00000000e+00 6.96406254e-02]\n",
            " [0.00000000e+00 9.84206917e-01]\n",
            " [0.00000000e+00 9.79870061e-01]] 0.2204025933890278\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 30)           3960      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 30)                7320      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 11,311\n",
            "Trainable params: 11,311\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "64/64 [==============================] - 4s 65ms/step - loss: 0.0520 - val_loss: 0.0521\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 0.0507 - val_loss: 0.0503\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0502 - val_loss: 0.0502\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0501 - val_loss: 0.0506\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0499 - val_loss: 0.0501\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 3s 52ms/step - loss: 0.0505 - val_loss: 0.0501\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0501 - val_loss: 0.0503\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 3s 51ms/step - loss: 0.0502 - val_loss: 0.0505\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0500 - val_loss: 0.0520\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0500 - val_loss: 0.0501\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0500 - val_loss: 0.0501\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0498 - val_loss: 0.0502\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 3s 55ms/step - loss: 0.0499 - val_loss: 0.0503\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0498 - val_loss: 0.0506\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0499 - val_loss: 0.0500\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0497 - val_loss: 0.0501\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0498 - val_loss: 0.0500\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0498 - val_loss: 0.0498\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0494 - val_loss: 0.0499\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0491 - val_loss: 0.0491\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0483 - val_loss: 0.0476\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 0.0483 - val_loss: 0.0491\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0490 - val_loss: 0.0499\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0487 - val_loss: 0.0479\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0471 - val_loss: 0.0466\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0433 - val_loss: 0.0485\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0502 - val_loss: 0.0470\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0390 - val_loss: 0.0472\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0259 - val_loss: 0.0170\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0145 - val_loss: 0.0110\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 0.0082 - val_loss: 0.0075\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0085 - val_loss: 0.0074\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0047 - val_loss: 0.0054\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 4s 58ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0046 - val_loss: 0.0040\n",
            "Epoch 37/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 0.0036 - val_loss: 0.0032\n",
            "Epoch 38/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0030 - val_loss: 0.0055\n",
            "Epoch 39/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0029 - val_loss: 0.0055\n",
            "Epoch 40/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 41/100\n",
            "64/64 [==============================] - 4s 58ms/step - loss: 0.0025 - val_loss: 0.0032\n",
            "Epoch 42/100\n",
            "64/64 [==============================] - 4s 59ms/step - loss: 0.0034 - val_loss: 0.0026\n",
            "Epoch 43/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 44/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 45/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 46/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 47/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0026 - val_loss: 0.0041\n",
            "Epoch 48/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 49/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 50/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0024 - val_loss: 0.0026\n",
            "Epoch 51/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 52/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 53/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 54/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 55/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 56/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 57/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 58/100\n",
            "64/64 [==============================] - 3s 52ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 59/100\n",
            "64/64 [==============================] - 3s 52ms/step - loss: 9.5459e-04 - val_loss: 0.0016\n",
            "Epoch 60/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 61/100\n",
            "64/64 [==============================] - 3s 52ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 62/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 63/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 8.9682e-04 - val_loss: 9.9440e-04\n",
            "Epoch 64/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 0.0011 - val_loss: 9.3962e-04\n",
            "Epoch 65/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 66/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 9.0301e-04 - val_loss: 0.0013\n",
            "Epoch 67/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 7.2557e-04 - val_loss: 8.3506e-04\n",
            "Epoch 68/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 7.0912e-04 - val_loss: 0.0010\n",
            "Epoch 69/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 7.0212e-04 - val_loss: 6.9340e-04\n",
            "Epoch 70/100\n",
            "64/64 [==============================] - 4s 60ms/step - loss: 7.3817e-04 - val_loss: 7.4305e-04\n",
            "Epoch 71/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 7.9285e-04 - val_loss: 7.2381e-04\n",
            "Epoch 72/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 7.5813e-04 - val_loss: 0.0012\n",
            "Epoch 73/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 9.1960e-04 - val_loss: 9.1684e-04\n",
            "Epoch 74/100\n",
            "64/64 [==============================] - 3s 55ms/step - loss: 9.6512e-04 - val_loss: 9.4081e-04\n",
            "Epoch 75/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 5.7255e-04 - val_loss: 6.0629e-04\n",
            "Epoch 76/100\n",
            "64/64 [==============================] - 4s 57ms/step - loss: 9.0559e-04 - val_loss: 8.6013e-04\n",
            "Epoch 77/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 6.0419e-04 - val_loss: 0.0012\n",
            "Epoch 78/100\n",
            "64/64 [==============================] - 4s 58ms/step - loss: 9.8793e-04 - val_loss: 0.0014\n",
            "Epoch 79/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 6.2687e-04 - val_loss: 6.3811e-04\n",
            "Epoch 80/100\n",
            "64/64 [==============================] - 3s 55ms/step - loss: 6.1798e-04 - val_loss: 0.0012\n",
            "Epoch 81/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 6.0442e-04 - val_loss: 0.0018\n",
            "Epoch 82/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 6.7291e-04 - val_loss: 8.4347e-04\n",
            "Epoch 83/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 6.2696e-04 - val_loss: 6.5986e-04\n",
            "Epoch 84/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 5.7496e-04 - val_loss: 5.3024e-04\n",
            "Epoch 85/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 7.6418e-04 - val_loss: 0.0011\n",
            "Epoch 86/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 5.2721e-04 - val_loss: 5.0351e-04\n",
            "Epoch 87/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 7.0349e-04 - val_loss: 5.1481e-04\n",
            "Epoch 88/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 7.6838e-04 - val_loss: 4.8412e-04\n",
            "Epoch 89/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 4.5650e-04 - val_loss: 4.6708e-04\n",
            "Epoch 90/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 4.5464e-04 - val_loss: 7.3897e-04\n",
            "Epoch 91/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 4.0859e-04 - val_loss: 5.3805e-04\n",
            "Epoch 92/100\n",
            "64/64 [==============================] - 4s 56ms/step - loss: 4.6707e-04 - val_loss: 9.3578e-04\n",
            "Epoch 93/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 6.7763e-04 - val_loss: 5.0852e-04\n",
            "Epoch 94/100\n",
            "64/64 [==============================] - 3s 54ms/step - loss: 6.8484e-04 - val_loss: 7.0147e-04\n",
            "Epoch 95/100\n",
            "64/64 [==============================] - 4s 60ms/step - loss: 6.9451e-04 - val_loss: 0.0012\n",
            "Epoch 96/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 8.0205e-04 - val_loss: 6.7929e-04\n",
            "Epoch 97/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 5.5389e-04 - val_loss: 4.3713e-04\n",
            "Epoch 98/100\n",
            "64/64 [==============================] - 3s 53ms/step - loss: 4.2768e-04 - val_loss: 5.1390e-04\n",
            "Epoch 99/100\n",
            "64/64 [==============================] - 4s 55ms/step - loss: 5.6701e-04 - val_loss: 8.9709e-04\n",
            "Epoch 100/100\n",
            "64/64 [==============================] - 3s 52ms/step - loss: 6.2500e-04 - val_loss: 3.4861e-04\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 3.2112e-04\n",
            "0.4055581308949056 \t 0.40908325 \tdiff: 0.0035251162898478072\n",
            "0.470973203337257 \t 0.47095373 \tdiff: 1.9470608298877146e-05\n",
            "0.5901529116865212 \t 0.5846625 \tdiff: 0.005490414642911556\n",
            "0.09608048851806217 \t 0.04216886 \tdiff: 0.05391162912565761\n",
            "0.5323143244598334 \t 0.52471215 \tdiff: 0.0076021791313116704\n",
            "correctness: 95.9090909090909 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5qbEjq1GNqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a6a54307-d360-4a7c-94f0-f5fca12d65fc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmgmtPEsJFEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}